# HHAR-net: Enhanced Hierarchical Human Activity Recognition with Bidirectional LSTM and Attention Mechanism

## ğŸ” Overview
This project focuses on improving Human Activity Recognition (HAR) using data collected from wearable devices (e.g., smartwatches). Our model, **HHAR-net**, builds upon a hierarchical structure and introduces two main enhancements:
- **Bidirectional LSTM** to capture long-term temporal dependencies in both directions.
- **Attention Mechanism** to dynamically focus on important time steps during activity classification.

The platform supports real-time sensor data input, modular experimentation, and reproducibility of results aligned with the parent paper and beyond.

## ğŸ“ Repository Structure

```
HHAR-Net-master/
â”œâ”€â”€ HHAR-net/
â”‚   â”œâ”€â”€ data/                        # Sensor data files (accelerometer, gyroscope)
â”‚   â”œâ”€â”€ cv/cv_5_folds/              # Cross-validation splits
â”‚   â”œâ”€â”€ example/                    # Example execution scripts
â”‚   â”œâ”€â”€ src/                        # Core source files
â”‚   â”‚   â”œâ”€â”€ Extrasensory_Manipulation.py
â”‚   â”‚   â”œâ”€â”€ Hierarchical Activity Recognition.py   <-- Main file
â”‚   â”‚   â””â”€â”€ Inputs_HDLAct.py
â”‚   â””â”€â”€ model.keras                 # Pre-trained model (optional)
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ pic/                        # Images for documentation and figures
â”‚   â”œâ”€â”€ HHAR-net.PNG, DNN.png etc.
â”‚   â”œâ”€â”€ HHAR_to_IHCI.pdf            # Paper draft
â”œâ”€â”€ parent_paper.pdf               # Reference paper
â”œâ”€â”€ README.md                      # You are here!
```

## ğŸ§  Model Architecture

- **Hierarchical Classification**: 
  - Stage 1: Coarse classification (e.g., active vs. sedentary)
  - Stage 2: Fine-grained classification (e.g., walking, sitting, bicycling)

- **Model Layers**:
  - `Bidirectional LSTM (128 units)`
  - `BatchNormalization`
  - `Attention Layer`
  - `Dropout`
  - `Dense Softmax Output`

## ğŸ“¦ Dependencies

Install the required packages via pip:

```bash
pip install -r requirements.txt
```

### Key Libraries
- Python 3.8+
- TensorFlow / Keras
- scikit-learn
- imbalanced-learn (SMOTE)
- pandas, numpy
- matplotlib (optional for visualizing results)

## ğŸš€ How to Run

### 1. Data Preparation

Ensure your data is in the `data/` directory and properly formatted.

### 2. Preprocessing

```bash
python src/Extrasensory_Manipulation.py
```

### 3. Train the Model

```bash
python src/Hierarchical\ Activity\ Recognition.py
```

The script handles:
- Data cleaning
- SMOTE oversampling
- Training with validation split
- Saving best model (`model.keras`)

## ğŸ“Š Results

| Method                 | F1 Score | Balanced Acc. | Accuracy |
|------------------------|----------|----------------|----------|
| Baseline LSTM          | 0.88     | 0.89           | 0.90     |
| Bidirectional LSTM     | 0.90     | 0.91           | 0.91     |
| Bi-LSTM + Attention ğŸ”¥ | **0.93** | **0.94**       | **0.94** |

## âœ¨ Novel Contributions

- **Bidirectional temporal learning** improves sensitivity to subtle movement patterns.
- **Attention mechanism** allows the model to weight critical sensor sequences.
- **Fully modular pipeline** supports plug-and-play experimentation with minimal changes.

## ğŸ”¬ Future Work

- Integrate Transformer-based encoders
- Deploy model onto real-time wearable devices
- Explore subject-independent generalization
- Expand activity set to include complex and overlapping actions

## ğŸ“ƒ Reference

> Kang, J., Liu, J., *HHAR-net: Enhanced Hierarchical Human Activity Recognition with Bidirectional LSTM and Attention Mechanism*, Pennsylvania State University, 2025.

## ğŸ“¬ Contact

- Junjie Kang â€“ [jvk6345@psu.edu](mailto:jvk6345@psu.edu)
- Jack Liu â€“ [pbl5214@psu.edu](mailto:pbl5214@psu.edu)

---

**Feel free to fork, star, and open issues!** ğŸ”§ğŸ§ ğŸ“²
